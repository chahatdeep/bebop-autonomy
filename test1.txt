Ready to fly...
Taking off...


Iteration 0
Getting current state
[-0.76700664 -0.17456031  3.23707008  2.49143076]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.7472759806575734, -0.2060694089607758, 3.2607471850064296, 2.45027126063413]
Getting reward
Position Reward: 0.691922
Setting target values
[-0.34614068  0.23793766  1.66839814  0.27419344  0.56060237 -0.19410776
  0.90995425  0.05556725]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 1
Getting current state
[-0.74727601 -0.20606941  3.26074719  2.45027137]
Getting predicted q_values
Getting best action
Moving quadcopter
Picking random action
Moving RGT
Getting new state
[-0.7354515389160912, -0.23953693605003262, 3.2449013873899393, 2.483338048385061]
Getting reward
Position Reward: 0.679414
Setting target values
[-0.34195006  0.2508896   1.66897297  0.2821283   0.55992579 -0.18711428
  0.90171832  0.03659515]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 2
Getting current state
[-0.73545152 -0.23953694  3.24490142  2.48333812]
Getting predicted q_values
Getting best action
Moving quadcopter
Picking random action
Moving FWD
Getting new state
[-0.7116041759875712, -0.25971203368376927, 3.1852751313717005, 2.4825866360450815]
Getting reward
Position Reward: 0.671368
Setting target values
[-0.36307698  0.25327629  1.66309798  0.27102759  0.55690491 -0.18416916
  0.9046014   0.03827564]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 3
Getting current state
[-0.71160418 -0.25971204  3.18527508  2.48258662]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.6482784188777518, -0.3033299958561771, 3.17407343001866, 2.4522838067701915]
Getting reward
Position Reward: 0.653084
Setting target values
[-0.37015772  0.254087    1.63662159  0.25791511  0.54804516 -0.17733721
  0.89742333  0.04040723]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 4
Getting current state
[-0.64827842 -0.30333     3.17407346  2.45228386]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.591466580900023, -0.3524417932765834, 3.2415985172491757, 2.437532911417709]
Getting reward
Position Reward: 0.631802
Setting target values
[-0.36918277  0.27354568  1.63067126  0.26359287  0.54199696 -0.16412686
  0.88339585  0.01794921]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 5
Getting current state
[-0.59146661 -0.35244179  3.24159861  2.4375329 ]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.5371068393267949, -0.390646796183162, 3.3467297366874087, 2.3989315911044082]
Getting reward
Position Reward: 0.615590
Setting target values
[-0.3714186   0.30006418  1.64957035  0.28433469  0.54578239 -0.15519221
  0.87803018 -0.01418109]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 6
Getting current state
[-0.53710681 -0.39064679  3.34672976  2.3989315 ]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.4907750638426289, -0.4320550433201668, 3.450650798082233, 2.4254951718328415]
Getting reward
Position Reward: 0.598836
Setting target values
[-0.36106312  0.33058426  1.67582846  0.31817099  0.55402994 -0.14748062
  0.87146205 -0.05478205]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 7
Getting current state
[-0.49077508 -0.43205506  3.45065069  2.42549515]
Getting predicted q_values
Getting best action
Moving quadcopter
Picking random action
Moving UP
Getting new state
[-0.4668328623606971, -0.49408776068318344, 3.5006141788541103, 2.4225172666529957]
Getting reward
Position Reward: 0.575514
Setting target values
[-0.37183952  0.35552475  1.69456816  0.34289584  0.56420684 -0.14461188
  0.8770045  -0.08275856]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 8
Getting current state
[-0.46683288 -0.49408776  3.50061417  2.4225173 ]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.4499385854315146, -0.48770739136995306, 3.5551013707375825, 2.42178879760095]
Getting reward
Position Reward: 0.578206
Setting target values
[-0.3879742   0.37328634  1.72155392  0.34835592  0.5660876  -0.13774903
  0.87628961 -0.1041614 ]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 9
Getting current state
[-0.44960335 -0.48781452  3.55582738  2.42019296]
Getting predicted q_values
Getting best action
Moving quadcopter
Picking random action
Moving BCK
Getting new state
[-0.47805931911319355, -0.5176020942335656, 3.62110107231596, 2.4290080027159013]
Getting reward
Position Reward: 0.567174
Setting target values
[-0.3794986   0.38289502  1.72830486  0.36706689  0.57275224 -0.13870065
  0.87758201 -0.11685385]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 10
Getting current state
[-0.47805932 -0.51760209  3.62110114  2.42900801]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.5308963418445019, -0.5551163889155362, 3.68843639377669, 2.419016017558656]
Getting reward
Position Reward: 0.554567
Setting target values
[-0.39373577  0.38645229  1.72905052  0.36983919  0.57971203 -0.14172076
  0.88653916 -0.12337263]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 11
Getting current state
[-0.53089637 -0.55511642  3.68843651  2.41901612]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.5332343432786788, -0.5938338521868599, 3.7721967578952182, 2.4298701225923995]
Getting reward
Position Reward: 0.543962
Setting target values
[-0.40886813  0.38670143  1.72715187  0.36801463  0.58629972 -0.14519887
  0.8952117  -0.12888201]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 12
Getting current state
[-0.53323436 -0.59383386  3.77219677  2.42987013]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.5135769241329546, -0.6334667098101787, 3.884723366813347, 2.384352456323063]
Getting reward
Position Reward: 0.535030
Setting target values
[-0.42225093  0.39905664  1.74329472  0.37910923  0.59456635 -0.14558728
  0.90253735 -0.14499713]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 13
Getting current state
[-0.51357692 -0.63346672  3.88472342  2.38435245]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.49125913078753664, -0.6802585064723413, 3.9907882061566453, 2.4018455959006277]
Getting reward
Position Reward: 0.526535
Setting target values
[-0.41505879  0.42315519  1.77175558  0.40743217  0.60403079 -0.1410154
  0.89939815 -0.18043245]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 14
Getting current state
[-0.49125913 -0.68025851  3.99078822  2.40184569]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.4567386699664485, -0.4763151838515472, 4.212120337945657, 2.3626943087424]
Getting reward
Position Reward: 0.582224
Setting target values
[-0.42859423  0.44403973  1.86488569  0.42716947  0.61429274 -0.13946749
  0.90632647 -0.20557667]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 15
Getting current state
[-0.45673868 -0.47631517  4.21212053  2.36269426]
Getting predicted q_values
Getting best action
Moving quadcopter
Picking random action
Moving DWN
Getting new state
[-0.4120595171951661, -0.0030837046059283366, 4.502968139669789, 2.333858513465732]
Getting reward
Position Reward: 0.734109
Setting target values
[-0.32433587  0.45775458  2.06305456  0.53284991  0.65115023 -0.16274671
  0.91241932 -0.23291422]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 16
Getting current state
[-0.41012493  0.03723463  4.52159023  2.34184027]
Getting predicted q_values
Getting best action
Moving quadcopter
Moving LFT
Getting new state
[-0.3882345954602484, 0.15167879850917304, 4.544306074236112, 2.1850836422060502]
Getting reward
Position Reward: 0.713142
Setting target values
[-0.10331605  0.45044583  2.08570361  0.72215903  0.71720785 -0.22080331
  0.92947555 -0.23836733]
Computing loss
Loss: 0.000000
Backpropagation


Iteration 17
Getting current state
[-0.38823459  0.151